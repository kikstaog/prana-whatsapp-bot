#!/usr/bin/env python3
"""
Ollama Setup Script for Prana Juice Bar WhatsApp Bot
Helps install and configure Ollama for enhanced bot functionality
"""

import os
import sys
import subprocess
import requests
import json
import time
from typing import Optional

def check_ollama_installed() -> bool:
    """Check if Ollama is installed"""
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def install_ollama_windows():
    """Install Ollama on Windows"""
    print("ğŸ”„ Installing Ollama on Windows...")
    
    # Download and install Ollama
    install_url = "https://ollama.ai/download/ollama-windows-amd64.exe"
    
    try:
        # Download installer
        print("ğŸ“¥ Downloading Ollama installer...")
        response = requests.get(install_url, stream=True)
        installer_path = "ollama-installer.exe"
        
        with open(installer_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        # Run installer
        print("ğŸ”§ Running Ollama installer...")
        subprocess.run([installer_path, '/S'], check=True)
        
        # Clean up
        os.remove(installer_path)
        
        print("âœ… Ollama installed successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error installing Ollama: {e}")
        print("\nğŸ“‹ Manual installation instructions:")
        print("1. Visit https://ollama.ai/download")
        print("2. Download the Windows installer")
        print("3. Run the installer and follow the prompts")
        print("4. Restart your terminal/command prompt")
        return False

def install_ollama_mac():
    """Install Ollama on macOS"""
    print("ğŸ”„ Installing Ollama on macOS...")
    
    try:
        # Use curl to install Ollama
        install_command = "curl -fsSL https://ollama.ai/install.sh | sh"
        subprocess.run(install_command, shell=True, check=True)
        
        print("âœ… Ollama installed successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error installing Ollama: {e}")
        print("\nğŸ“‹ Manual installation instructions:")
        print("1. Visit https://ollama.ai/download")
        print("2. Download the macOS installer")
        print("3. Run the installer and follow the prompts")
        return False

def install_ollama_linux():
    """Install Ollama on Linux"""
    print("ğŸ”„ Installing Ollama on Linux...")
    
    try:
        # Use curl to install Ollama
        install_command = "curl -fsSL https://ollama.ai/install.sh | sh"
        subprocess.run(install_command, shell=True, check=True)
        
        print("âœ… Ollama installed successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error installing Ollama: {e}")
        print("\nğŸ“‹ Manual installation instructions:")
        print("1. Visit https://ollama.ai/download")
        print("2. Follow the Linux installation instructions")
        print("3. Or run: curl -fsSL https://ollama.ai/install.sh | sh")
        return False

def start_ollama_service():
    """Start Ollama service"""
    print("ğŸš€ Starting Ollama service...")
    
    try:
        # Start Ollama
        subprocess.run(['ollama', 'serve'], start_new_session=True)
        
        # Wait a moment for service to start
        time.sleep(3)
        
        # Test connection
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… Ollama service started successfully!")
            return True
        else:
            print("âš ï¸ Ollama service may not be running properly")
            return False
            
    except Exception as e:
        print(f"âŒ Error starting Ollama service: {e}")
        return False

def download_model(model_name: str = "llama2") -> bool:
    """Download Ollama model"""
    print(f"ğŸ“¥ Downloading {model_name} model...")
    
    try:
        # Download the model
        subprocess.run(['ollama', 'pull', model_name], check=True)
        print(f"âœ… {model_name} model downloaded successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error downloading {model_name} model: {e}")
        return False

def test_ollama_connection() -> bool:
    """Test Ollama connection and model availability"""
    print("ğŸ” Testing Ollama connection...")
    
    try:
        # Test basic connection
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            print("âŒ Cannot connect to Ollama service")
            return False
        
        # Check available models
        models = response.json().get('models', [])
        model_names = [model['name'] for model in models]
        
        print(f"ğŸ“‹ Available models: {model_names}")
        
        # Test with llama2 model
        if 'llama2' in model_names:
            print("âœ… llama2 model is available")
            return True
        else:
            print("âš ï¸ llama2 model not found, downloading...")
            return download_model("llama2")
            
    except Exception as e:
        print(f"âŒ Error testing Ollama connection: {e}")
        return False

def test_model_response() -> bool:
    """Test model response generation"""
    print("ğŸ§ª Testing model response...")
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama2",
                "prompt": "Hola, Â¿cÃ³mo estÃ¡s?",
                "stream": False
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if result.get('response'):
                print("âœ… Model response test successful!")
                return True
            else:
                print("âŒ No response from model")
                return False
        else:
            print(f"âŒ Model test failed with status: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Error testing model response: {e}")
        return False

def main():
    """Main setup function"""
    print("ğŸ¤– Ollama Setup for Prana Juice Bar WhatsApp Bot")
    print("=" * 60)
    
    # Check if Ollama is already installed
    if check_ollama_installed():
        print("âœ… Ollama is already installed")
    else:
        print("ğŸ“¦ Ollama not found, installing...")
        
        # Detect OS and install
        if sys.platform.startswith('win'):
            if not install_ollama_windows():
                print("âŒ Failed to install Ollama")
                return
        elif sys.platform.startswith('darwin'):
            if not install_ollama_mac():
                print("âŒ Failed to install Ollama")
                return
        else:
            if not install_ollama_linux():
                print("âŒ Failed to install Ollama")
                return
    
    # Start Ollama service
    if not start_ollama_service():
        print("âŒ Failed to start Ollama service")
        return
    
    # Test connection
    if not test_ollama_connection():
        print("âŒ Failed to connect to Ollama")
        return
    
    # Test model response
    if not test_model_response():
        print("âŒ Failed to test model response")
        return
    
    print("\nğŸ‰ Ollama setup completed successfully!")
    print("\nğŸ“‹ Next steps:")
    print("1. Run the enhanced bot: python enhanced_whatsapp_bot.py")
    print("2. The bot will automatically use Ollama for natural responses")
    print("3. If Ollama fails, it will fallback to the rule-based system")
    print("\nğŸ’¡ Tips:")
    print("- Keep Ollama running in the background")
    print("- You can use different models: llama2, llama2:7b, llama2:13b")
    print("- The bot will automatically detect if Ollama is available")

if __name__ == "__main__":
    main() 